{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1LPa7kmApbxQZEo3qeyFd1av_ZgNwPvaV",
      "authorship_tag": "ABX9TyNqgJkA87dlqYdO4o4XSgjL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zzanggyu/Pill-AI-Notification-System/blob/main/vgg.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install ultralytics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U-Bc6yYiKG3p",
        "outputId": "1d597182-233f-4c46-ff18-4e0518dcce07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.3.20-py3-none-any.whl.metadata (34 kB)\n",
            "Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.26.4)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (3.7.1)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.10.0.84)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (10.4.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.13.1)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.4.1+cu121)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.19.1+cu121)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.66.5)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.2.2)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.13.2)\n",
            "Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n",
            "  Downloading ultralytics_thop-2.0.9-py3-none-any.whl.metadata (9.3 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2024.8.30)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.4.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2024.6.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Downloading ultralytics-8.3.20-py3-none-any.whl (876 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m876.6/876.6 kB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ultralytics_thop-2.0.9-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: ultralytics-thop, ultralytics\n",
            "Successfully installed ultralytics-8.3.20 ultralytics-thop-2.0.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorflow opencv-python"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mjk7SfdnnmGv",
        "outputId": "0dc85034-1ad0-42bb-cf30-b9d1687f49cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.0)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (13.9.2)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.13.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (3.0.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TensorFlow (일반적으로 Colab에 이미 설치되어 있습니다)\n",
        "!pip install tensorflow\n",
        "\n",
        "# OpenCV\n",
        "!pip install opencv-python-headless\n",
        "\n",
        "# scikit-learn\n",
        "!pip install scikit-learn\n",
        "\n",
        "# matplotlib\n",
        "!pip install matplotlib\n",
        "\n",
        "# tqdm\n",
        "!pip install tqdm\n",
        "\n",
        "# Pillow (PIL) - 이미지 처리용\n",
        "!pip install Pillow\n",
        "\n",
        "# (선택사항) 추가적인 데이터 처리 및 시각화 라이브러리\n",
        "!pip install pandas seaborn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "opb6kCcCuBF6",
        "outputId": "85ebffec-df13-415f-bb03-014003488455"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.0)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (13.9.2)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.13.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (3.0.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n",
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python-headless) (1.26.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.5)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (10.4.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (0.13.2)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /usr/local/lib/python3.10/dist-packages (from seaborn) (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# OpenCV\n",
        "!pip install opencv-python-headless\n",
        "\n",
        "# tqdm (진행 바 표시용)\n",
        "!pip install tqdm\n",
        "\n",
        "# scikit-learn (이미 설치되어 있을 가능성이 높지만, 확실히 하기 위해)\n",
        "!pip install scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rQlf8PChBbTS",
        "outputId": "da5d0adb-f8af-4c65-e429-a9956b91b6c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python-headless) (1.26.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VGG 학습"
      ],
      "metadata": {
        "id": "_LuIZINWt2hr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##################### 이걸로\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import models, transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import json\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 모델 정의\n",
        "def create_model():\n",
        "    model = models.vgg16(weights='IMAGENET1K_V1')\n",
        "\n",
        "    # 특징 추출 부분 고정\n",
        "    for param in model.features.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    # 분류기 부분 수정\n",
        "    model.classifier[6] = nn.Linear(4096, 4)\n",
        "\n",
        "    return model\n",
        "\n",
        "# 데이터셋 클래스\n",
        "class PillShapeDataset(Dataset):\n",
        "    def __init__(self, data, img_dir, transform=None):\n",
        "        self.data = data\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.shape_mapping = {\n",
        "            \"Circle\": \"원형\",\n",
        "            \"Oblong\": \"장방형\",\n",
        "            \"Oval\": \"타원형\",\n",
        "            \"Octagon\": \"팔각형\"\n",
        "        }\n",
        "        self.valid_shapes = list(self.shape_mapping.values())\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.data[idx]['filename']\n",
        "        img_path = os.path.join(self.img_dir, img_name)\n",
        "\n",
        "        try:\n",
        "            image = Image.open(img_path).convert('RGB')\n",
        "        except FileNotFoundError:\n",
        "            print(f\"파일을 찾을 수 없습니다: {img_path}\")\n",
        "            image = Image.new('RGB', (224, 224), color='black')\n",
        "\n",
        "        try:\n",
        "            english_shape = self.data[idx]['regions'][0]['region_attributes']['shape']\n",
        "            shape = self.shape_mapping.get(english_shape)\n",
        "            if shape is None:\n",
        "                print(f\"알 수 없는 모양: {english_shape}, 파일: {img_name}\")\n",
        "                label = -1\n",
        "            else:\n",
        "                label = self.valid_shapes.index(shape)\n",
        "        except (KeyError, IndexError):\n",
        "            print(f\"shape 정보 없음, 파일: {img_name}\")\n",
        "            label = -1\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "# 데이터 로드 함수\n",
        "def load_data(json_path, img_dir):\n",
        "    # JSON 데이터 로드\n",
        "    with open(json_path, 'r', encoding='utf-8') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    # 데이터 처리\n",
        "    processed_data = []\n",
        "    for k, v in data['_via_img_metadata'].items():\n",
        "        item = {'filename': v['filename'], 'regions': v.get('regions', [])}\n",
        "        if item['regions']:\n",
        "            processed_data.append(item)\n",
        "\n",
        "    # 존재하는 파일만 필터링\n",
        "    existing_files = set(os.listdir(img_dir))\n",
        "    filtered_data = [item for item in processed_data if item['filename'] in existing_files]\n",
        "\n",
        "    return filtered_data\n",
        "\n",
        "# 학습 함수\n",
        "def train_model(model, criterion, optimizer, train_loader, val_loader, num_epochs=25):\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
        "        print('-' * 10)\n",
        "\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()\n",
        "                dataloader = train_loader\n",
        "            else:\n",
        "                model.eval()\n",
        "                dataloader = val_loader\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "            total_valid = 0\n",
        "\n",
        "            for inputs, labels in dataloader:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum((preds == labels) & (labels != -1))\n",
        "                total_valid += torch.sum(labels != -1)\n",
        "\n",
        "            epoch_loss = running_loss / len(dataloader.dataset)\n",
        "            epoch_acc = running_corrects.double() / total_valid if total_valid > 0 else 0\n",
        "\n",
        "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
        "\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                torch.save({\n",
        "                    'epoch': epoch,\n",
        "                    'model_state_dict': model.state_dict(),\n",
        "                    'optimizer_state_dict': optimizer.state_dict(),\n",
        "                    'loss': epoch_loss,\n",
        "                }, '/content/drive/MyDrive/캡스톤 디자인 최종본/vgg/pill_shape_classifier_best.pth')\n",
        "\n",
        "    return model\n",
        "\n",
        "def main():\n",
        "    # 경로 설정\n",
        "    json_path = '/content/drive/MyDrive/캡스톤 디자인 최종본/vgg/Merged_VGG_Learning_Data.json'\n",
        "    img_dir = '/content/drive/MyDrive/캡스톤 디자인 최종본/vgg/Pill_img'\n",
        "\n",
        "    # 데이터 로드\n",
        "    data = load_data(json_path, img_dir)\n",
        "    train_data, val_data = train_test_split(data, test_size=0.2, random_state=42)\n",
        "\n",
        "    # 데이터 변환 정의\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    # 데이터셋 생성\n",
        "    train_dataset = PillShapeDataset(train_data, img_dir, transform=transform)\n",
        "    val_dataset = PillShapeDataset(val_data, img_dir, transform=transform)\n",
        "\n",
        "    # 데이터로더 생성\n",
        "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "    # 모델 생성\n",
        "    model = create_model()\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=-1)\n",
        "    optimizer = optim.Adam(model.classifier.parameters(), lr=0.001)\n",
        "\n",
        "    # 학습 실행\n",
        "    trained_model = train_model(model, criterion, optimizer, train_loader, val_loader)\n",
        "\n",
        "    # 최종 모델 저장\n",
        "    torch.save({\n",
        "        'model_state_dict': trained_model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "    }, '/content/drive/MyDrive/캡스톤 디자인 최종본/vgg/pill_shape_classifier_final.pth')\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 906
        },
        "id": "ykm06Tfo5tj0",
        "outputId": "8b0474cf-0e23-4abd-da66-8a5c7e2e5f45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "----------\n",
            "train Loss: 0.6055 Acc: 0.8156\n",
            "val Loss: 0.2523 Acc: 0.9579\n",
            "Epoch 2/25\n",
            "----------\n",
            "train Loss: 0.4135 Acc: 0.9402\n",
            "val Loss: 0.1857 Acc: 0.9635\n",
            "Epoch 3/25\n",
            "----------\n",
            "train Loss: 0.4064 Acc: 0.9514\n",
            "val Loss: 0.7354 Acc: 0.9185\n",
            "Epoch 4/25\n",
            "----------\n",
            "train Loss: 0.6593 Acc: 0.9444\n",
            "val Loss: 0.2268 Acc: 0.9775\n",
            "Epoch 5/25\n",
            "----------\n",
            "train Loss: 0.6357 Acc: 0.9620\n",
            "val Loss: 0.2931 Acc: 0.9747\n",
            "Epoch 6/25\n",
            "----------\n",
            "train Loss: 0.2639 Acc: 0.9782\n",
            "val Loss: 0.0931 Acc: 0.9916\n",
            "Epoch 7/25\n",
            "----------\n",
            "train Loss: 0.2880 Acc: 0.9824\n",
            "val Loss: 0.6075 Acc: 0.9635\n",
            "Epoch 8/25\n",
            "----------\n",
            "train Loss: 0.4481 Acc: 0.9831\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-f54c34ca6d69>\u001b[0m in \u001b[0;36m<cell line: 183>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-18-f54c34ca6d69>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0;31m# 학습 실행\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m     \u001b[0mtrained_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;31m# 최종 모델 저장\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-f54c34ca6d69>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, criterion, optimizer, train_loader, val_loader, num_epochs)\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0mtotal_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m                 \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                 \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-f54c34ca6d69>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m             \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RGB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"파일을 찾을 수 없습니다: {img_path}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[1;32m    993\u001b[0m             \u001b[0mdeprecate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    994\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 995\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    996\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    997\u001b[0m         \u001b[0mhas_transparency\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"transparency\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/ImageFile.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m                             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m                             \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m                                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## 팔각형 제외 학습 원형,타원형,장방형만 학습\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import models, transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import json\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 모델 정의\n",
        "def create_model():\n",
        "    model = models.vgg16(weights='IMAGENET1K_V1')\n",
        "\n",
        "    # 특징 추출 부분 고정\n",
        "    for param in model.features.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    # 분류기 부분 수정\n",
        "    model.classifier[6] = nn.Linear(4096, 3)  # 3개의 클래스로 변경\n",
        "\n",
        "    return model\n",
        "\n",
        "# 데이터셋 클래스\n",
        "class PillShapeDataset(Dataset):\n",
        "    def __init__(self, data, img_dir, transform=None):\n",
        "        self.data = data\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.shape_mapping = {\n",
        "            \"Circle\": \"원형\",\n",
        "            \"Oblong\": \"장방형\",\n",
        "            \"Oval\": \"타원형\"\n",
        "            # Octagon 제거\n",
        "        }\n",
        "        self.valid_shapes = list(self.shape_mapping.values())\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.data[idx]['filename']\n",
        "        img_path = os.path.join(self.img_dir, img_name)\n",
        "\n",
        "        try:\n",
        "            image = Image.open(img_path).convert('RGB')\n",
        "        except FileNotFoundError:\n",
        "            print(f\"파일을 찾을 수 없습니다: {img_path}\")\n",
        "            image = Image.new('RGB', (224, 224), color='black')\n",
        "\n",
        "        try:\n",
        "            english_shape = self.data[idx]['regions'][0]['region_attributes']['shape']\n",
        "            shape = self.shape_mapping.get(english_shape)\n",
        "            if shape is None:\n",
        "                print(f\"알 수 없는 모양: {english_shape}, 파일: {img_name}\")\n",
        "                label = -1\n",
        "            else:\n",
        "                label = self.valid_shapes.index(shape)\n",
        "        except (KeyError, IndexError):\n",
        "            print(f\"shape 정보 없음, 파일: {img_name}\")\n",
        "            label = -1\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "# 데이터 로드 함수\n",
        "def load_data(json_path, img_dir):\n",
        "    # JSON 데이터 로드\n",
        "    with open(json_path, 'r', encoding='utf-8') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    # 데이터 처리 (팔각형 제외)\n",
        "    processed_data = []\n",
        "    valid_shapes = [\"Circle\", \"Oblong\", \"Oval\"]\n",
        "\n",
        "    for k, v in data['_via_img_metadata'].items():\n",
        "        if v.get('regions'):\n",
        "            shape = v['regions'][0]['region_attributes'].get('shape')\n",
        "            if shape in valid_shapes:\n",
        "                processed_data.append({\n",
        "                    'filename': v['filename'],\n",
        "                    'regions': v['regions']\n",
        "                })\n",
        "\n",
        "    # 존재하는 파일만 필터링\n",
        "    existing_files = set(os.listdir(img_dir))\n",
        "    filtered_data = [item for item in processed_data if item['filename'] in existing_files]\n",
        "\n",
        "    # 데이터 통계 출력\n",
        "    print(\"데이터 통계:\")\n",
        "    shape_counts = {}\n",
        "    for item in filtered_data:\n",
        "        shape = item['regions'][0]['region_attributes']['shape']\n",
        "        shape_counts[shape] = shape_counts.get(shape, 0) + 1\n",
        "\n",
        "    print(f\"총 데이터 수: {len(filtered_data)}\")\n",
        "    print(\"클래스별 데이터 수:\")\n",
        "    for shape, count in shape_counts.items():\n",
        "        print(f\"{shape}: {count}\")\n",
        "\n",
        "    return filtered_data\n",
        "\n",
        "# 학습 함수\n",
        "def train_model(model, criterion, optimizer, train_loader, val_loader, num_epochs=25):\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
        "        print('-' * 10)\n",
        "\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()\n",
        "                dataloader = train_loader\n",
        "            else:\n",
        "                model.eval()\n",
        "                dataloader = val_loader\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "            total_valid = 0\n",
        "\n",
        "            for inputs, labels in dataloader:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum((preds == labels) & (labels != -1))\n",
        "                total_valid += torch.sum(labels != -1)\n",
        "\n",
        "            epoch_loss = running_loss / len(dataloader.dataset)\n",
        "            epoch_acc = running_corrects.double() / total_valid if total_valid > 0 else 0\n",
        "\n",
        "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
        "\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                torch.save({\n",
        "                    'epoch': epoch,\n",
        "                    'model_state_dict': model.state_dict(),\n",
        "                    'optimizer_state_dict': optimizer.state_dict(),\n",
        "                    'loss': epoch_loss,\n",
        "                }, '/content/drive/MyDrive/캡스톤 디자인 최종본/vgg/pill_shape_classifier_3class_best.pth')\n",
        "\n",
        "    return model\n",
        "\n",
        "def main():\n",
        "    # 경로 설정\n",
        "    json_path = '/content/drive/MyDrive/캡스톤 디자인 최종본/vgg/Merged_VGG_Learning_Data.json'\n",
        "    img_dir = '/content/drive/MyDrive/캡스톤 디자인 최종본/vgg/Pill_img'\n",
        "\n",
        "    # 데이터 로드\n",
        "    data = load_data(json_path, img_dir)\n",
        "    train_data, val_data = train_test_split(data, test_size=0.2, random_state=42)\n",
        "\n",
        "    # 데이터 변환 정의\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    # 데이터셋 생성\n",
        "    train_dataset = PillShapeDataset(train_data, img_dir, transform=transform)\n",
        "    val_dataset = PillShapeDataset(val_data, img_dir, transform=transform)\n",
        "\n",
        "    # 데이터로더 생성\n",
        "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "    # 모델 생성\n",
        "    model = create_model()\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=-1)\n",
        "    optimizer = optim.Adam(model.classifier.parameters(), lr=0.001)\n",
        "\n",
        "    # 학습 실행\n",
        "    trained_model = train_model(model, criterion, optimizer, train_loader, val_loader)\n",
        "\n",
        "    # 최종 모델 저장\n",
        "    torch.save({\n",
        "        'model_state_dict': trained_model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "    }, '/content/drive/MyDrive/캡스톤 디자인 최종본/vgg/pill_shape_classifier_3class_final.pth')\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "GsnhFUcXNf68",
        "outputId": "b18e6f08-eb2e-49dd-dc43-a81f5180bb28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "데이터 통계:\n",
            "총 데이터 수: 1333\n",
            "클래스별 데이터 수:\n",
            "Oval: 443\n",
            "Oblong: 445\n",
            "Circle: 445\n",
            "Epoch 1/25\n",
            "----------\n",
            "train Loss: 0.6926 Acc: 0.7814\n",
            "val Loss: 0.6218 Acc: 0.8951\n",
            "Epoch 2/25\n",
            "----------\n",
            "train Loss: 0.1843 Acc: 0.9615\n",
            "val Loss: 0.1630 Acc: 0.9700\n",
            "Epoch 3/25\n",
            "----------\n",
            "train Loss: 0.3276 Acc: 0.9615\n",
            "val Loss: 0.0564 Acc: 0.9888\n",
            "Epoch 4/25\n",
            "----------\n",
            "train Loss: 0.1584 Acc: 0.9812\n",
            "val Loss: 0.4492 Acc: 0.9625\n",
            "Epoch 5/25\n",
            "----------\n",
            "train Loss: 0.2299 Acc: 0.9794\n",
            "val Loss: 1.3139 Acc: 0.9700\n",
            "Epoch 6/25\n",
            "----------\n",
            "train Loss: 0.7169 Acc: 0.9653\n",
            "val Loss: 1.2516 Acc: 0.9700\n",
            "Epoch 7/25\n",
            "----------\n",
            "train Loss: 0.3410 Acc: 0.9841\n",
            "val Loss: 0.5951 Acc: 0.9775\n",
            "Epoch 8/25\n",
            "----------\n",
            "train Loss: 0.3336 Acc: 0.9822\n",
            "val Loss: 0.1877 Acc: 0.9888\n",
            "Epoch 9/25\n",
            "----------\n",
            "train Loss: 0.6308 Acc: 0.9756\n",
            "val Loss: 0.3784 Acc: 0.9888\n",
            "Epoch 10/25\n",
            "----------\n",
            "train Loss: 0.5382 Acc: 0.9803\n",
            "val Loss: 0.1977 Acc: 0.9888\n",
            "Epoch 11/25\n",
            "----------\n",
            "train Loss: 0.0525 Acc: 0.9953\n",
            "val Loss: 1.0794 Acc: 0.9775\n",
            "Epoch 12/25\n",
            "----------\n",
            "train Loss: 0.3278 Acc: 0.9897\n",
            "val Loss: 0.9087 Acc: 0.9813\n",
            "Epoch 13/25\n",
            "----------\n",
            "train Loss: 0.3410 Acc: 0.9925\n",
            "val Loss: 0.2427 Acc: 0.9963\n",
            "Epoch 14/25\n",
            "----------\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-312f545d4b83>\u001b[0m in \u001b[0;36m<cell line: 202>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-38-312f545d4b83>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[0;31m# 학습 실행\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m     \u001b[0mtrained_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;31m# 최종 모델 저장\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-38-312f545d4b83>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, criterion, optimizer, train_loader, val_loader, num_epochs)\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0mtotal_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m                 \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m                 \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-38-312f545d4b83>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    352\u001b[0m             \u001b[0mPIL\u001b[0m \u001b[0mImage\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRescaled\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m         \"\"\"\n\u001b[0;32m--> 354\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mantialias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation, max_size, antialias)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Anti-alias option is always applied for PIL Image input. Argument antialias is ignored.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0mpil_interpolation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpil_modes_mapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF_pil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpil_interpolation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mF_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mantialias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mantialias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/_functional_pil.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Got inappropriate size arg: {size}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(self, size, resample, box, reducing_gap)\u001b[0m\n\u001b[1;32m   2326\u001b[0m                 )\n\u001b[1;32m   2327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2328\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbox\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2330\u001b[0m     def reduce(\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# vgg 테스트"
      ],
      "metadata": {
        "id": "L1wHVR4Lt6Ky"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##################### 최종\n",
        "\n",
        "import torch\n",
        "from torchvision import models, transforms\n",
        "from PIL import Image\n",
        "import torch.nn as nn\n",
        "import os\n",
        "\n",
        "def create_model():\n",
        "    model = models.vgg16(weights='IMAGENET1K_V1')\n",
        "\n",
        "    # 분류기 부분 수정\n",
        "    model.classifier[6] = nn.Linear(4096, 3)  # 3개의 클래스\n",
        "\n",
        "    return model\n",
        "\n",
        "def load_trained_model(model_path):\n",
        "    # 모델 생성\n",
        "    model = create_model()\n",
        "\n",
        "    # GPU 사용 가능 여부 확인\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # 저장된 모델 불러오기\n",
        "    checkpoint = torch.load(model_path, map_location=device)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "    # 모델을 평가 모드로 설정\n",
        "    model.eval()\n",
        "    model = model.to(device)\n",
        "\n",
        "    return model, device\n",
        "\n",
        "def predict_image(model, image_path, device):\n",
        "    # 이미지 변환을 위한 전처리\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    # 클래스 레이블 정의\n",
        "    classes = [\"원형\", \"장방형\", \"타원형\"]\n",
        "\n",
        "    # 이미지 로드 및 전처리\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    image_tensor = transform(image).unsqueeze(0).to(device)\n",
        "\n",
        "    # 예측\n",
        "    with torch.no_grad():\n",
        "        outputs = model(image_tensor)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "        # 각 클래스에 대한 확률 계산\n",
        "        probabilities = torch.nn.functional.softmax(outputs, dim=1)[0]\n",
        "\n",
        "    return {\n",
        "        'predicted_class': classes[predicted.item()],\n",
        "        'probabilities': {classes[i]: prob.item() for i, prob in enumerate(probabilities)}\n",
        "    }\n",
        "\n",
        "def test_model_on_directory(model_path, test_dir):\n",
        "    # 모델 로드\n",
        "    model, device = load_trained_model(model_path)\n",
        "\n",
        "    # 테스트 디렉토리의 모든 이미지에 대해 예측 수행\n",
        "    results = []\n",
        "\n",
        "    for image_name in os.listdir(test_dir):\n",
        "        if image_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "            image_path = os.path.join(test_dir, image_name)\n",
        "            try:\n",
        "                prediction = predict_image(model, image_path, device)\n",
        "                results.append({\n",
        "                    'image_name': image_name,\n",
        "                    'prediction': prediction\n",
        "                })\n",
        "                print(f\"\\n이미지: {image_name}\")\n",
        "                print(f\"예측된 클래스: {prediction['predicted_class']}\")\n",
        "                print(\"클래스별 확률:\")\n",
        "                for class_name, prob in prediction['probabilities'].items():\n",
        "                    print(f\"{class_name}: {prob:.4f}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {image_name}: {str(e)}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# 사용 예시\n",
        "if __name__ == \"__main__\":\n",
        "    # # 모델 경로와 테스트 이미지 디렉토리 설정\n",
        "    # model_path = '/content/drive/MyDrive/캡스톤 디자인 최종본/vgg/pill_shape_classifier_3class_best.pth'\n",
        "    # test_dir = '/content/drive/MyDrive/캡스톤 디자인 최종본/pillimgtest/realtest'\n",
        "\n",
        "    # # 전체 테스트 디렉토리에 대한 테스트 실행\n",
        "    # results = test_model_on_directory(model_path, test_dir)\n",
        "\n",
        "    # 단일 이미지 테스트를 위한 예시\n",
        "    model, device = load_trained_model(model_path)\n",
        "    single_result = predict_image(model, '/content/drive/MyDrive/캡스톤 디자인 최종본/pillimgtest/손바닥/KakaoTalk_20241009_134400839_07.jpg', device)\n",
        "    print(single_result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZBEFOwub8Kle",
        "outputId": "da0a8d9a-4581-4493-9741-bfd692155ac3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'predicted_class': '타원형', 'probabilities': {'원형': 0.0, '장방형': 0.0, '타원형': 1.0}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "######\n",
        "import torch\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "import cv2\n",
        "import numpy as np\n",
        "from ultralytics import YOLO\n",
        "\n",
        "class PillDetectionClassification:\n",
        "    def __init__(self, yolo_model_path, vgg_model_path):\n",
        "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        # YOLO 모델 로드\n",
        "        self.yolo_model = YOLO(yolo_model_path)\n",
        "\n",
        "        # VGG 모델 로드\n",
        "        self.vgg_model = self.create_vgg_model()\n",
        "        checkpoint = torch.load(vgg_model_path, map_location=self.device)\n",
        "        self.vgg_model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        self.vgg_model.eval()\n",
        "        self.vgg_model = self.vgg_model.to(self.device)\n",
        "\n",
        "        # Transform 정의\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "        self.shape_classes = [\"원형\", \"장방형\", \"타원형\"]\n",
        "\n",
        "    def create_vgg_model(self):\n",
        "        model = models.vgg16(weights='IMAGENET1K_V1')\n",
        "        model.classifier[6] = nn.Linear(4096, 3)\n",
        "        return model\n",
        "\n",
        "    def classify_pill_shape(self, image):\n",
        "        # PIL Image로 변환\n",
        "        if isinstance(image, np.ndarray):\n",
        "            image = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "        elif not isinstance(image, Image.Image):\n",
        "            raise ValueError(\"Unsupported image type\")\n",
        "\n",
        "        # 이미지 전처리\n",
        "        image_tensor = self.transform(image).unsqueeze(0).to(self.device)\n",
        "\n",
        "        # 예측\n",
        "        with torch.no_grad():\n",
        "            outputs = self.vgg_model(image_tensor)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            probabilities = torch.nn.functional.softmax(outputs, dim=1)[0]\n",
        "\n",
        "        return {\n",
        "            'predicted_class': self.shape_classes[predicted.item()],\n",
        "            'probabilities': {\n",
        "                self.shape_classes[i]: prob.item()\n",
        "                for i, prob in enumerate(probabilities)\n",
        "            }\n",
        "        }\n",
        "\n",
        "    def detect_and_classify_pills(self, image_path, conf_threshold=0.3):\n",
        "        # 이미지 로드\n",
        "        original_image = cv2.imread(image_path)\n",
        "        if original_image is None:\n",
        "            raise ValueError(f\"Could not load image at {image_path}\")\n",
        "\n",
        "        # 전체 이미지에 대한 형태 분류\n",
        "        image_pil = Image.open(image_path).convert('RGB')\n",
        "        shape_prediction = self.classify_pill_shape(image_pil)\n",
        "        print(f\"\\nOverall image shape classification: {shape_prediction['predicted_class']}\")\n",
        "        print(\"Shape probabilities:\", shape_prediction['probabilities'])\n",
        "\n",
        "        # YOLO로 알약 위치 감지\n",
        "        results = self.yolo_model.predict(image_path, conf=conf_threshold)\n",
        "\n",
        "        detected_pills = []\n",
        "\n",
        "        for result in results:\n",
        "            boxes = result.boxes\n",
        "            print(f\"\\nYOLO detected {len(boxes)} pills\")\n",
        "\n",
        "            for box in boxes:\n",
        "                # 바운딩 박스 좌표\n",
        "                x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
        "                confidence = float(box.conf[0])\n",
        "\n",
        "                # 결과 저장\n",
        "                detected_pills.append({\n",
        "                    'bbox': (x1, y1, x2, y2),\n",
        "                    'confidence': confidence,\n",
        "                    'shape': shape_prediction['predicted_class'],\n",
        "                    'shape_probabilities': shape_prediction['probabilities']\n",
        "                })\n",
        "\n",
        "                # 결과 시각화\n",
        "                cv2.rectangle(original_image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "                label = f\"{shape_prediction['predicted_class']} ({confidence:.2f})\"\n",
        "                cv2.putText(original_image, label, (x1, y1-10),\n",
        "                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
        "\n",
        "        return original_image, detected_pills\n",
        "\n",
        "def main():\n",
        "    # 모델 경로 설정\n",
        "    yolo_model_path = '/content/drive/MyDrive/캡스톤 디자인 최종본/Yolov8n/weights/best.pt'\n",
        "    vgg_model_path = '/content/drive/MyDrive/캡스톤 디자인 최종본/vgg/pill_shape_classifier_3class_best.pth'\n",
        "\n",
        "    # 테스트 이미지 경로\n",
        "    image_path = '/content/drive/MyDrive/캡스톤 디자인 최종본/pillimgtest/realtest/타이레놀1.jpg'\n",
        "\n",
        "    try:\n",
        "        # 모델 초기화\n",
        "        detector = PillDetectionClassification(yolo_model_path, vgg_model_path)\n",
        "\n",
        "        # 알약 감지 및 분류 실행\n",
        "        result_image, detected_pills = detector.detect_and_classify_pills(image_path)\n",
        "\n",
        "        print(\"\\nDetection Results:\")\n",
        "        for i, pill in enumerate(detected_pills, 1):\n",
        "            print(f\"\\nPill {i}:\")\n",
        "            print(f\"Location: {pill['bbox']}\")\n",
        "            print(f\"Detection confidence: {pill['confidence']:.4f}\")\n",
        "            print(f\"Shape: {pill['shape']}\")\n",
        "            print(\"Shape probabilities:\", pill['shape_probabilities'])\n",
        "\n",
        "        # 결과 이미지 저장\n",
        "        output_path = 'result.jpg'\n",
        "        cv2.imwrite(output_path, result_image)\n",
        "        print(f\"\\n결과 이미지가 {output_path}에 저장되었습니다.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"에러 발생: {str(e)}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2fwJtEniJuFT",
        "outputId": "12337c6d-332b-456a-e763-c1154bba153b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Overall image shape classification: 장방형\n",
            "Shape probabilities: {'원형': 0.0, '장방형': 1.0, '타원형': 0.0}\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/캡스톤 디자인 최종본/pillimgtest/realtest/타이레놀1.jpg: 640x480 1 pill, 7.5ms\n",
            "Speed: 3.0ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "YOLO detected 1 pills\n",
            "\n",
            "Detection Results:\n",
            "\n",
            "Pill 1:\n",
            "Location: (918, 1773, 2081, 2540)\n",
            "Detection confidence: 0.7895\n",
            "Shape: 장방형\n",
            "Shape probabilities: {'원형': 0.0, '장방형': 1.0, '타원형': 0.0}\n",
            "\n",
            "결과 이미지가 result.jpg에 저장되었습니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import models, transforms\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_recall_fscore_support\n",
        "import json\n",
        "import os\n",
        "\n",
        "# 데이터셋 클래스 정의\n",
        "class PillShapeDataset(Dataset):\n",
        "    def __init__(self, data, img_dir, transform=None):\n",
        "        self.data = data\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.shape_mapping = {\n",
        "            \"Circle\": \"원형\",\n",
        "            \"Oblong\": \"장방형\",\n",
        "            \"Oval\": \"타원형\",\n",
        "            \"Octagon\": \"팔각형\"\n",
        "        }\n",
        "        self.valid_shapes = list(self.shape_mapping.values())\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.data[idx]['filename']\n",
        "        img_path = os.path.join(self.img_dir, img_name)\n",
        "        try:\n",
        "            image = Image.open(img_path).convert('RGB')\n",
        "        except FileNotFoundError:\n",
        "            print(f\"파일을 찾을 수 없습니다: {img_path}\")\n",
        "            image = Image.new('RGB', (224, 224), color='black')\n",
        "\n",
        "        try:\n",
        "            english_shape = self.data[idx]['regions'][0]['region_attributes']['shape']\n",
        "            shape = self.shape_mapping.get(english_shape)\n",
        "            if shape is None:\n",
        "                print(f\"알 수 없는 모양: {english_shape}, 파일: {img_name}\")\n",
        "                label = -1\n",
        "            else:\n",
        "                label = self.valid_shapes.index(shape)\n",
        "        except (KeyError, IndexError):\n",
        "            print(f\"shape 정보 없음, 파일: {img_name}\")\n",
        "            label = -1\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "# 데이터 변환 정의\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# JSON 데이터 로드\n",
        "def load_json_data(json_path):\n",
        "    with open(json_path, 'r', encoding='utf-8') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    processed_data = []\n",
        "    for k, v in data['_via_img_metadata'].items():\n",
        "        item = {'filename': v['filename'], 'regions': v.get('regions', [])}\n",
        "        if item['regions']:\n",
        "            processed_data.append(item)\n",
        "\n",
        "    return processed_data\n",
        "\n",
        "# VGG 모델 로드 및 설정\n",
        "def load_model(model_path):\n",
        "    model = models.vgg16(weights=None)\n",
        "    num_ftrs = model.classifier[6].in_features\n",
        "    model.classifier[6] = nn.Linear(num_ftrs, 4)\n",
        "\n",
        "    checkpoint = torch.load(model_path)\n",
        "    if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    else:\n",
        "        model.load_state_dict(checkpoint)\n",
        "\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "    return model, device\n",
        "\n",
        "# 평가 함수\n",
        "def evaluate_dataset(model, test_loader, device):\n",
        "    predictions = []\n",
        "    actual = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            # -1 레이블(잘못된 데이터)은 제외\n",
        "            mask = labels != -1\n",
        "            predictions.extend(preds[mask].cpu().numpy())\n",
        "            actual.extend(labels[mask].cpu().numpy())\n",
        "\n",
        "    return np.array(predictions), np.array(actual)\n",
        "\n",
        "# 혼동 행렬 시각화\n",
        "def plot_confusion_matrix(cm, classes):\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=classes, yticklabels=classes)\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.show()\n",
        "\n",
        "# 메인 평가 코드\n",
        "def main():\n",
        "    # 경로 설정\n",
        "    model_path = '/content/drive/MyDrive/캡스톤 디자인 최종본/vgg/pill_shape_classifier.pth'\n",
        "    json_path = '/content/drive/MyDrive/캡스톤 디자인 최종본/vgg/Merged_VGG_Learning_Data.json'\n",
        "    img_dir = '/content/drive/MyDrive/캡스톤 디자인 최종본/vgg/Pill_img'\n",
        "\n",
        "    # 클래스 이름 정의\n",
        "    classes = [\"원형\", \"장방형\", \"타원형\", \"팔각형\"]\n",
        "\n",
        "    # 모델 로드\n",
        "    print(\"모델 로딩 중...\")\n",
        "    model, device = load_model(model_path)\n",
        "    print(f\"모델이 {device}에 로드되었습니다.\")\n",
        "\n",
        "    # 테스트 데이터셋 로드\n",
        "    print(\"테스트 데이터 로딩 중...\")\n",
        "    test_data = load_json_data(json_path)\n",
        "    test_dataset = PillShapeDataset(test_data, img_dir, transform=transform)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "    print(f\"테스트 데이터 크기: {len(test_dataset)}\")\n",
        "\n",
        "    # 전체 데이터셋 평가\n",
        "    print(\"\\n테스트 세트 평가 중...\")\n",
        "    predictions, actual = evaluate_dataset(model, test_loader, device)\n",
        "\n",
        "    # 평가 지표 계산 및 출력\n",
        "    accuracy = accuracy_score(actual, predictions)\n",
        "    print(f\"\\n전체 정확도: {accuracy:.4f}\")\n",
        "\n",
        "    print(\"\\n클래스별 성능 지표:\")\n",
        "    print(classification_report(actual, predictions, target_names=classes))\n",
        "\n",
        "    # 혼동 행렬 계산 및 시각화\n",
        "    cm = confusion_matrix(actual, predictions)\n",
        "    plot_confusion_matrix(cm, classes)\n",
        "\n",
        "    # 단일 이미지 테스트\n",
        "    test_image_path = '/content/drive/MyDrive/캡스톤 디자인 최종본/pillimgtest/realtest/잘되는 거/글로게이트정1.jpg'\n",
        "    if os.path.exists(test_image_path):\n",
        "        image = Image.open(test_image_path).convert('RGB')\n",
        "        image_tensor = transform(image).unsqueeze(0).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(image_tensor)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            print(f\"\\n테스트 이미지 예측 결과: {classes[predicted.item()]}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "rgdgqxcJwOhK",
        "outputId": "ac67e7af-350c-412a-bde2-5fe08286a21a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "모델 로딩 중...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-c79887bda120>:82: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(model_path)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "모델이 cuda:0에 로드되었습니다.\n",
            "테스트 데이터 로딩 중...\n",
            "테스트 데이터 크기: 1779\n",
            "\n",
            "테스트 세트 평가 중...\n",
            "shape 정보 없음, 파일: 200400100_020.jpg\n",
            "\n",
            "전체 정확도: 0.9989\n",
            "\n",
            "클래스별 성능 지표:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          원형       1.00      1.00      1.00       445\n",
            "         장방형       1.00      1.00      1.00       446\n",
            "         타원형       1.00      1.00      1.00       443\n",
            "         팔각형       1.00      1.00      1.00       444\n",
            "\n",
            "    accuracy                           1.00      1778\n",
            "   macro avg       1.00      1.00      1.00      1778\n",
            "weighted avg       1.00      1.00      1.00      1778\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/seaborn/utils.py:61: UserWarning: Glyph 50896 (\\N{HANGUL SYLLABLE WEON}) missing from current font.\n",
            "  fig.canvas.draw()\n",
            "/usr/local/lib/python3.10/dist-packages/seaborn/utils.py:61: UserWarning: Glyph 54805 (\\N{HANGUL SYLLABLE HYEONG}) missing from current font.\n",
            "  fig.canvas.draw()\n",
            "/usr/local/lib/python3.10/dist-packages/seaborn/utils.py:61: UserWarning: Glyph 51109 (\\N{HANGUL SYLLABLE JANG}) missing from current font.\n",
            "  fig.canvas.draw()\n",
            "/usr/local/lib/python3.10/dist-packages/seaborn/utils.py:61: UserWarning: Glyph 48169 (\\N{HANGUL SYLLABLE BANG}) missing from current font.\n",
            "  fig.canvas.draw()\n",
            "/usr/local/lib/python3.10/dist-packages/seaborn/utils.py:61: UserWarning: Glyph 53440 (\\N{HANGUL SYLLABLE TA}) missing from current font.\n",
            "  fig.canvas.draw()\n",
            "/usr/local/lib/python3.10/dist-packages/seaborn/utils.py:61: UserWarning: Glyph 54036 (\\N{HANGUL SYLLABLE PAL}) missing from current font.\n",
            "  fig.canvas.draw()\n",
            "/usr/local/lib/python3.10/dist-packages/seaborn/utils.py:61: UserWarning: Glyph 44033 (\\N{HANGUL SYLLABLE GAG}) missing from current font.\n",
            "  fig.canvas.draw()\n",
            "/usr/local/lib/python3.10/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 50896 (\\N{HANGUL SYLLABLE WEON}) missing from current font.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.10/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 54805 (\\N{HANGUL SYLLABLE HYEONG}) missing from current font.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.10/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 51109 (\\N{HANGUL SYLLABLE JANG}) missing from current font.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.10/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 48169 (\\N{HANGUL SYLLABLE BANG}) missing from current font.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.10/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 53440 (\\N{HANGUL SYLLABLE TA}) missing from current font.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.10/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 54036 (\\N{HANGUL SYLLABLE PAL}) missing from current font.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.10/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 44033 (\\N{HANGUL SYLLABLE GAG}) missing from current font.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x800 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "테스트 이미지 예측 결과: 원형\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ultralytics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lEBIJaBN3GDP",
        "outputId": "59b05d53-d8e4-4879-d4eb-0779729b34f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.3.19-py3-none-any.whl.metadata (34 kB)\n",
            "Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.26.4)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (3.7.1)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.10.0.84)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (10.4.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.13.1)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.4.1+cu121)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.19.1+cu121)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.66.5)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.2.2)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.13.2)\n",
            "Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n",
            "  Downloading ultralytics_thop-2.0.9-py3-none-any.whl.metadata (9.3 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2024.8.30)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.4.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2024.6.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Downloading ultralytics-8.3.19-py3-none-any.whl (876 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m876.6/876.6 kB\u001b[0m \u001b[31m55.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ultralytics_thop-2.0.9-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: ultralytics-thop, ultralytics\n",
            "Successfully installed ultralytics-8.3.19 ultralytics-thop-2.0.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "############### yolo로 좌표대로 잘라서 vgg에 넘겨 모양 예측 테스트\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "from ultralytics import YOLO\n",
        "\n",
        "def create_model():\n",
        "    model = models.vgg16(weights=None)\n",
        "    # 분류기 부분 수정\n",
        "    num_ftrs = model.classifier[6].in_features\n",
        "    model.classifier[6] = nn.Linear(num_ftrs, 4)\n",
        "    return model\n",
        "\n",
        "def detect_and_classify_pill(image_path, yolo_model_path, vgg_model_path):\n",
        "    # YOLO 모델 로드\n",
        "    yolo_model = YOLO(yolo_model_path)\n",
        "\n",
        "    # VGG 모델 로드\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    vgg_model = create_model()\n",
        "    checkpoint = torch.load(vgg_model_path)\n",
        "    vgg_model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    vgg_model.to(device)\n",
        "    vgg_model.eval()\n",
        "\n",
        "    # 이미지 변환기\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    # 이미지 로드\n",
        "    image = cv2.imread(image_path)\n",
        "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # YOLO로 알약 검출\n",
        "    results = yolo_model(image_rgb)\n",
        "\n",
        "    # 각 검출된 알약에 대해 처리\n",
        "    detections = []\n",
        "\n",
        "    for r in results:\n",
        "        boxes = r.boxes\n",
        "        for box in boxes:\n",
        "            # 바운딩 박스 좌표 가져오기\n",
        "            x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
        "\n",
        "            # 이미지 자르기\n",
        "            cropped_img = image_rgb[y1:y2, x1:x2]\n",
        "\n",
        "            # PIL Image로 변환\n",
        "            pil_img = Image.fromarray(cropped_img)\n",
        "\n",
        "            # 자른 이미지 저장 (디버깅용)\n",
        "            debug_path = f'cropped_{len(detections)}.jpg'\n",
        "            pil_img.save(debug_path)\n",
        "            print(f\"자른 이미지 저장됨: {debug_path}\")\n",
        "\n",
        "            # VGG 모델용으로 변환\n",
        "            image_tensor = transform(pil_img).unsqueeze(0).to(device)\n",
        "\n",
        "            # VGG로 모양 분류\n",
        "            with torch.no_grad():\n",
        "                outputs = vgg_model(image_tensor)\n",
        "                probabilities = torch.nn.functional.softmax(outputs, dim=1)\n",
        "                confidence, predicted = torch.max(probabilities, 1)\n",
        "\n",
        "                shapes = [\"원형\", \"장방형\", \"타원형\",]\n",
        "                probs = probabilities[0].cpu().numpy()\n",
        "\n",
        "                # 결과 저장\n",
        "                detection = {\n",
        "                    'box': (x1, y1, x2, y2),\n",
        "                    'shape': shapes[predicted.item()],\n",
        "                    'confidence': confidence.item(),\n",
        "                    'probabilities': {shape: float(prob) for shape, prob in zip(shapes, probs)}\n",
        "                }\n",
        "                detections.append(detection)\n",
        "\n",
        "                # 결과 출력\n",
        "                print(f\"\\n검출된 알약 {len(detections)}:\")\n",
        "                print(f\"위치: ({x1}, {y1}, {x2}, {y2})\")\n",
        "                print(\"\\n각 클래스별 확률:\")\n",
        "                for shape, prob in zip(shapes, probs):\n",
        "                    print(f\"{shape}: {prob:.4f}\")\n",
        "                print(f\"\\n예측된 모양: {shapes[predicted.item()]}\")\n",
        "                print(f\"신뢰도: {confidence.item():.2%}\")\n",
        "                print(\"-\" * 50)\n",
        "\n",
        "    # 결과 시각화\n",
        "    image_with_boxes = image_rgb.copy()\n",
        "    for det in detections:\n",
        "        x1, y1, x2, y2 = det['box']\n",
        "        cv2.rectangle(image_with_boxes, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "        label = f\"{det['shape']} ({det['confidence']:.2%})\"\n",
        "        cv2.putText(image_with_boxes, label, (x1, y1-10),\n",
        "                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
        "\n",
        "    # 결과 이미지 저장\n",
        "    result_path = 'result.jpg'\n",
        "    cv2.imwrite(result_path, cv2.cvtColor(image_with_boxes, cv2.COLOR_RGB2BGR))\n",
        "    print(f\"\\n결과 이미지 저장됨: {result_path}\")\n",
        "\n",
        "    return detections\n",
        "\n",
        "# 테스트 실행\n",
        "def main():\n",
        "    # 모델 경로\n",
        "    yolo_model_path = '/content/drive/MyDrive/캡스톤 디자인 최종본/Yolov8n/weights/best.pt'\n",
        "    vgg_model_path = '/content/drive/MyDrive/캡스톤 디자인 최종본/vgg/pill_shape_classifier_3class_best.pth'\n",
        "\n",
        "    # 테스트 이미지\n",
        "    image_path = '/content/drive/MyDrive/캡스톤 디자인 최종본/vgg/Pill_img/200400173_003.jpg'\n",
        "\n",
        "    print(\"알약 검출 및 분류 시작...\")\n",
        "    detections = detect_and_classify_pill(image_path, yolo_model_path, vgg_model_path)\n",
        "\n",
        "    print(f\"\\n총 {len(detections)}개의 알약이 검출되었습니다.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import torch.nn as nn\n",
        "    from torchvision import models\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "FvjxvlkiKXgu",
        "outputId": "8ecb3736-5190-4e01-9278-f2092cb8bea5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "알약 검출 및 분류 시작...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Error(s) in loading state_dict for VGG:\n\tsize mismatch for classifier.6.weight: copying a param with shape torch.Size([3, 4096]) from checkpoint, the shape in current model is torch.Size([4, 4096]).\n\tsize mismatch for classifier.6.bias: copying a param with shape torch.Size([3]) from checkpoint, the shape in current model is torch.Size([4]).",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-136701dcae08>\u001b[0m in \u001b[0;36m<cell line: 123>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-6-136701dcae08>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"알약 검출 및 분류 시작...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m     \u001b[0mdetections\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdetect_and_classify_pill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myolo_model_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvgg_model_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\n총 {len(detections)}개의 알약이 검출되었습니다.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-136701dcae08>\u001b[0m in \u001b[0;36mdetect_and_classify_pill\u001b[0;34m(image_path, yolo_model_path, vgg_model_path)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mvgg_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvgg_model_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mvgg_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_state_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mvgg_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mvgg_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2214\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2215\u001b[0;31m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0m\u001b[1;32m   2216\u001b[0m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[1;32m   2217\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for VGG:\n\tsize mismatch for classifier.6.weight: copying a param with shape torch.Size([3, 4096]) from checkpoint, the shape in current model is torch.Size([4, 4096]).\n\tsize mismatch for classifier.6.bias: copying a param with shape torch.Size([3]) from checkpoint, the shape in current model is torch.Size([4])."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## 이거다 ㅅㅂ\n",
        "\n",
        "import torch\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "import cv2\n",
        "import numpy as np\n",
        "from ultralytics import YOLO\n",
        "\n",
        "class PillDetectionClassification:\n",
        "    def __init__(self, yolo_model_path, vgg_model_path):\n",
        "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        # YOLO 모델 로드\n",
        "        self.yolo_model = YOLO(yolo_model_path)\n",
        "\n",
        "        # VGG 모델 로드 - 기존 코드와 동일한 방식 사용\n",
        "        self.vgg_model = self.create_vgg_model()\n",
        "        checkpoint = torch.load(vgg_model_path, map_location=self.device)\n",
        "        self.vgg_model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        self.vgg_model.eval()\n",
        "        self.vgg_model = self.vgg_model.to(self.device)\n",
        "\n",
        "        # 기존 코드와 동일한 transform 사용\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "        self.shape_classes = [\"원형\", \"장방형\", \"타원형\"]\n",
        "\n",
        "    def create_vgg_model(self):\n",
        "        model = models.vgg16(weights='IMAGENET1K_V1')\n",
        "        model.classifier[6] = nn.Linear(4096, 3)\n",
        "        return model\n",
        "\n",
        "    def classify_pill_shape(self, pill_image):\n",
        "        # PIL Image로 변환 확실히 하기\n",
        "        if isinstance(pill_image, np.ndarray):\n",
        "            pill_image = Image.fromarray(cv2.cvtColor(pill_image, cv2.COLOR_BGR2RGB))\n",
        "        elif not isinstance(pill_image, Image.Image):\n",
        "            raise ValueError(\"Unsupported image type\")\n",
        "\n",
        "        # 기존 코드와 동일한 전처리 방식 사용\n",
        "        image_tensor = self.transform(pill_image).unsqueeze(0).to(self.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.vgg_model(image_tensor)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            probabilities = torch.nn.functional.softmax(outputs, dim=1)[0]\n",
        "\n",
        "        return {\n",
        "            'predicted_class': self.shape_classes[predicted.item()],\n",
        "            'probabilities': {\n",
        "                self.shape_classes[i]: prob.item()\n",
        "                for i, prob in enumerate(probabilities)\n",
        "            }\n",
        "        }\n",
        "\n",
        "    def detect_and_classify_pills(self, image_path, conf_threshold=0.3):\n",
        "        # 원본 이미지 로드\n",
        "        original_image = cv2.imread(image_path)\n",
        "        if original_image is None:\n",
        "            raise ValueError(f\"Could not load image at {image_path}\")\n",
        "\n",
        "        # YOLO로 알약 감지\n",
        "        results = self.yolo_model.predict(image_path, conf=conf_threshold)\n",
        "\n",
        "        detected_pills = []\n",
        "\n",
        "        # 디버깅을 위한 출력 추가\n",
        "        print(f\"YOLO detected {len(results[0].boxes)} objects\")\n",
        "\n",
        "        for result in results:\n",
        "            boxes = result.boxes\n",
        "            for box in boxes:\n",
        "                x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
        "                confidence = float(box.conf[0])\n",
        "\n",
        "                # 잘라낸 이미지 확인\n",
        "                pill_image = original_image[y1:y2, x1:x2]\n",
        "                if pill_image.size == 0:\n",
        "                    print(f\"Warning: Empty crop at coordinates: ({x1}, {y1}, {x2}, {y2})\")\n",
        "                    continue\n",
        "\n",
        "                # 디버깅을 위한 출력\n",
        "                print(f\"Processing crop of size: {pill_image.shape}\")\n",
        "\n",
        "                # PIL Image로 변환\n",
        "                pill_image_rgb = cv2.cvtColor(pill_image, cv2.COLOR_BGR2RGB)\n",
        "                pill_image_pil = Image.fromarray(pill_image_rgb)\n",
        "\n",
        "                try:\n",
        "                    # VGG 모델로 분류\n",
        "                    shape_prediction = self.classify_pill_shape(pill_image_pil)\n",
        "\n",
        "                    # 결과 저장\n",
        "                    detected_pills.append({\n",
        "                        'bbox': (x1, y1, x2, y2),\n",
        "                        'confidence': confidence,\n",
        "                        'shape': shape_prediction['predicted_class'],\n",
        "                        'shape_probabilities': shape_prediction['probabilities']\n",
        "                    })\n",
        "\n",
        "                    # 결과 표시\n",
        "                    cv2.rectangle(original_image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "                    label = f\"{shape_prediction['predicted_class']} ({confidence:.2f})\"\n",
        "                    cv2.putText(original_image, label, (x1, y1-10),\n",
        "                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
        "\n",
        "                    # 디버깅을 위한 출력\n",
        "                    print(f\"Successfully classified pill: {shape_prediction['predicted_class']}\")\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing crop: {str(e)}\")\n",
        "                    continue\n",
        "\n",
        "        return original_image, detected_pills\n",
        "\n",
        "def main():\n",
        "    # 테스트용 함수\n",
        "    def test_single_image(detector, image_path):\n",
        "        # 원본 이미지로 직접 VGG 테스트 (YOLO 없이)\n",
        "        image = Image.open(image_path).convert('RGB')\n",
        "        direct_prediction = detector.classify_pill_shape(image)\n",
        "        print(\"\\nDirect VGG prediction (without YOLO):\")\n",
        "        print(direct_prediction)\n",
        "\n",
        "        # YOLO + VGG 파이프라인 테스트\n",
        "        result_image, detected_pills = detector.detect_and_classify_pills(image_path)\n",
        "        print(\"\\nYOLO + VGG pipeline results:\")\n",
        "        for pill in detected_pills:\n",
        "            print(f\"Shape: {pill['shape']}\")\n",
        "            print(f\"Confidence: {pill['confidence']:.4f}\")\n",
        "            print(\"Probabilities:\", pill['shape_probabilities'])\n",
        "\n",
        "        return result_image, detected_pills\n",
        "\n",
        "    # 모델 경로 설정\n",
        "    yolo_model_path = '/content/drive/MyDrive/캡스톤 디자인 최종본/Yolov8n/weights/best.pt'\n",
        "    vgg_model_path = '/content/drive/MyDrive/캡스톤 디자인 최종본/vgg/pill_shape_classifier_3class_best.pth'\n",
        "\n",
        "    # 모델 초기화\n",
        "    detector = PillDetectionClassification(yolo_model_path, vgg_model_path)\n",
        "\n",
        "    # 테스트 이미지\n",
        "    image_path = '/content/drive/MyDrive/캡스톤 디자인 최종본/pillimgtest/realtest/뮤테란캡슐1.jpg'\n",
        "\n",
        "    try:\n",
        "        result_image, detected_pills = test_single_image(detector, image_path)\n",
        "\n",
        "        # 결과 이미지 저장\n",
        "        output_path = 'result.jpg'\n",
        "        cv2.imwrite(output_path, result_image)\n",
        "        print(f\"\\n결과 이미지가 {output_path}에 저장되었습니다.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"에러 발생: {str(e)}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CzYFKu9r9Ntm",
        "outputId": "38b02d76-0156-42a7-b977-47353ec1d7a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Direct VGG prediction (without YOLO):\n",
            "{'predicted_class': '장방형', 'probabilities': {'원형': 0.0, '장방형': 1.0, '타원형': 0.0}}\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/캡스톤 디자인 최종본/pillimgtest/realtest/뮤테란캡슐1.jpg: 640x480 1 pill, 8.1ms\n",
            "Speed: 3.8ms preprocess, 8.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
            "YOLO detected 1 objects\n",
            "Processing crop of size: (743, 1148, 3)\n",
            "Successfully classified pill: 타원형\n",
            "\n",
            "YOLO + VGG pipeline results:\n",
            "Shape: 타원형\n",
            "Confidence: 0.7530\n",
            "Probabilities: {'원형': 0.0, '장방형': 0.0, '타원형': 1.0}\n",
            "\n",
            "결과 이미지가 result.jpg에 저장되었습니다.\n"
          ]
        }
      ]
    }
  ]
}